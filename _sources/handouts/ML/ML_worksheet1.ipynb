{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b3e546-296c-4d7e-b8c3-cac7280db97f",
   "metadata": {},
   "source": [
    "# Pipelines (preprocessing to estimator), CV, Gridsearch\n",
    "\n",
    "- [ ] how to do: preprocessing (with `ColumnTransformer`)\n",
    "- [ ] how to examine: preprecessing\n",
    "- [ ] how to do: pipelines + cross validation + scoring\n",
    "    - `make_pipeline()` with preprocessing and estimator\n",
    "    - examine pipeline elements \n",
    "    - fit and predict using the pipeline (not CV)\n",
    "    - examine the pipeline using CV (using the cv function and examining its output)\n",
    "- [ ] scoring vocab: recall/sensitivity, precision, specificity, accuracy, \n",
    "- [ ] how to do: optimizing a pipeline by \"tuning the hyperparameters\"\n",
    "    - hyperparameters are the parameters of functions/estimators in the steps in your pipeline\n",
    "    - set up and use `gridsearchCV`\n",
    "    - examine output of `gridsearchCV`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ccb8c-f508-4022-bbca-f3a03311a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.impute import SimpleImputer\n",
    "from df_after_transform import df_after_transform\n",
    "from sklearn.model_selection import KFold, cross_validate, GridSearchCV\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")  # display='text' is the default\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000, 'display.max_rows', 50, 'display.max_columns', None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cdaf8d-899f-41f5-b520-79da9c5284d5",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5771be6-0588-440b-b738-5a55f50bb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = pd.read_csv('inputs/2013_subsample.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81a81b-7e5d-4483-a987-90e2fd8c70d3",
   "metadata": {},
   "source": [
    "## Create the training and holdout samples\n",
    "\n",
    "Split your data into test and train. Your options:\n",
    "- [sklearn has some built in splitters](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "    - These are rarely the best optios for real world data. Prediction is often about the future!\n",
    "- [`test_train_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) can do basic splits, but may not be appropriate\n",
    "    - _test_ is the typical sklearn vernacular, on the website I call this this holdout sample\n",
    "- You can just keep the most recent time period of your samples in the holdout and put the rest into the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9807f9-3c31-48bd-b380-701d5f99b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to test and train (what we call the \"test\" subset here is the \"holdout\" data)\n",
    "\n",
    "# first let's separate y from X (as is typically done)\n",
    "y = loans.loan_status == 'Charged Off'\n",
    "y.value_counts()\n",
    "loans = loans.drop('loan_status',axis=1)\n",
    "\n",
    "# stratify will make sure that test/train both have equal fractions of outcome\n",
    "X_train, X_test, y_train, y_test = train_test_split(loans, y, stratify=y, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7e8a4-0496-4706-8c20-4ab4b4754642",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "On the **TRAINING DATA ONLY**: \n",
    "- do lots of EDA\n",
    "- look for missing values, which variables are what type, and outliers \n",
    "- figure out how you'd clean the data (imputation, scaling, encoding categorical vars)\n",
    "- these lessons will go into the preprocessinG portion of your pipeline \n",
    "- PRO TIP: `pandas-profiling` and `dabl` build automated reports. Which is nice, but remember you need to examine them closely! **There is no shortcut for EDA.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ecffc0-43bb-4b20-836e-4a9b72922635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport # now use ydata-profiling\n",
    "# \n",
    "# profile = ProfileReport(pd.concat([y_train, X_train], axis=1), \n",
    "#                         title='Lending Club Profiling Report',\n",
    "#                         html={'style':{'full_width':True}}) \n",
    "# profile.to_file(\"inputs/lending_club_EDA_training.html\") # can take a minute or two with this dataset size. Let's look at the one I uploaded...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da90522f-d069-4649-ad7c-32aa03ae3d4a",
   "metadata": {},
   "source": [
    "## NOW LET'S LEARN EACH PART OF \"Optimize a series of models\" FROM THE TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad9a34-752b-4282-a7d0-ce03e86ae572",
   "metadata": {},
   "source": [
    "### Steps 1 and 2: Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb4dc4-f531-4f95-be76-9e3e8e30b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline to clean each type of variable (1 pipe per var type)\n",
    "\n",
    "numer_pipe = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                           StandardScaler()) \n",
    "\n",
    "cat_pipe   = make_pipeline(OneHotEncoder(drop='first'))\n",
    "\n",
    "# combine those pipes into \"preprocess\" pipe\n",
    "\n",
    "preproc_pipe = ColumnTransformer(  \n",
    "    [ # arg 1 of ColumnTransformer is a list, so this starts the list\n",
    "    # a tuple for the numerical vars: name, pipe, which vars to apply to\n",
    "    (\"num_impute\", numer_pipe, ['annual_inc']),\n",
    "    # a tuple for the categorical vars: name, pipe, which vars to apply to\n",
    "    (\"cat_trans\", cat_pipe, ['grade'])\n",
    "    ]\n",
    "    , # ColumnTransformer can take other args, most important: \"remainder\"\n",
    "    remainder = 'drop' # you either drop or passthrough any vars not modified above\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9deeb97-af7d-4068-a75d-cecf2b36f5a1",
   "metadata": {},
   "source": [
    "_Note: You can have multiple \"numeric pipes\" or \"cat pipes\" if you think some number variables should receive different treatments, which is very common!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2b19a-887d-404f-b212-dcfb527b6bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# hot tip: check out what this preprocessing does before you continue!\n",
    "###########\n",
    "\n",
    "from df_after_transform import df_after_transform\n",
    "\n",
    "preproc_df = df_after_transform(preproc_pipe,X_train)\n",
    "print(f'There are {preproc_df.shape[1]} columns in the preprocessed data.')\n",
    "preproc_df.describe().T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c09ac5-fa1a-47fd-bd15-8af652ef4c16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXERCISES\n",
    "\n",
    "1. How many observations for `annual_inc` were non-missing before the processing? Does our imputation choice here we make a small or large impact?\n",
    "1. How many values of `grade` was there in the data before and after `preproc_pipe`? \n",
    "1. Above, revise `preproc_pipe` to include another continuous variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6dc05-99ea-49bd-87ad-d18072190b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0ae66-ac2e-4808-b4aa-46ba4991e356",
   "metadata": {},
   "source": [
    "### Prof demo: Fitting and using ONE model\n",
    "\n",
    "Warning: This is not best practice to do on the whole training sample. The point here is to simply show you how to estimate and use a model.\n",
    "\n",
    "_(The only time you fit a model on the whole training sample is the VERY end of the template, right before you check to see how it does on the holdout.)_\n",
    "\n",
    "Steps: https://ledatascifi.github.io/ledatascifi-2022/content/05/04c_onemodel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7cb00-5ba2-4c3a-a3f6-39a83f5656dc",
   "metadata": {},
   "source": [
    "To fit the model: `<model>.fit(X)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8880e9f2-2c72-4e84-b995-908fd503cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create (\"instantiate\") the estimator class with some hyperparameters,\n",
    "# note: the hypermeters are whatever is inside the \"()\"\n",
    "# assign this instance of the estimator to an object\n",
    "logit = LogisticRegression()\n",
    "\n",
    "# fit the model to the data: <model>.fit(X,y)\n",
    "# note: I'm only using annual income here for illustration\n",
    "logit.fit(X_train[['annual_inc']], \n",
    "          y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34622e51-07a4-4ebc-9b6b-e8ae940ee7ce",
   "metadata": {},
   "source": [
    "To use the model: `<model>.predict(X)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3fcc4-b5f2-4242-8781-2bdf71c4f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates predicted values \n",
    "y_pred = logit.predict(X_train[['annual_inc']],)\n",
    "\n",
    "print(f'''\n",
    "% predicted as charge offs: {round(100*(y_pred == 1).mean(),2)}\n",
    "Accuracy:                   {round(100*(y_pred == y_train).mean(),2)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38f9e05-1695-4711-a4bb-d5c4db4b8dcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXERCISES\n",
    "\n",
    "- Q4: Let's estimate a different logit model, and see if the accuracy or the number of predictions of charge offs changes. [This time change the penalty OR the value of \"C\".](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af2527-5d12-47d1-adb9-20fc836a3d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12edbc63-87fc-4c75-89ef-9ffa36142a90",
   "metadata": {},
   "source": [
    "### Steps 3, 4, and 5: Pipelines, CV, and model evaluation (scoring)\n",
    "\n",
    "Making a pipeline is easy: `make_pipeline` will put steps together for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77585a80-f73c-48b1-babd-7d6cef833911",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pipe = make_pipeline(preproc_pipe, LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81d990-05ff-4d1e-ab7e-a9e660669ee0",
   "metadata": {},
   "source": [
    "A pipeline is an object that stores its steps (and steps within steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b93e3-9e04-4e41-8e1a-c0506fbfa1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704d1f0-07fc-4834-b7c0-5e2776d95160",
   "metadata": {},
   "source": [
    "Fitting the model and using it is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7616d7-8203-4f3d-b5aa-8952212f8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pipe.fit(X_train, y_train) \n",
    "y_pred = logit_pipe.predict(X_train)\n",
    "\n",
    "print(f'''\n",
    "% predicted as charge offs: {round(100*(y_pred == 1).mean(),2)}\n",
    "Accuracy:                   {round(100*(y_pred == y_train).mean(),2)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247024f0-3bcb-4b66-a4a1-a8444eb530f4",
   "metadata": {},
   "source": [
    "But the better idea is to see how that model does on different \"folds\" of the data: CROSS VALIDATION.\n",
    "\n",
    "The [`cross_validate` (function docs here)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html?highlight=cross_validate#sklearn.model_selection.cross_validate) makes this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c4666-6da0-4d77-8de4-0a8b19ab9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(logit_pipe,\n",
    "                        X_train, y_train,\n",
    "                        scoring='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd878ec-0312-4fb2-8b9b-cab993ec233f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXERCISES\n",
    "\n",
    "- Q5: What is the average score of this model in the folds?\n",
    "- Q6: How many folds did we use above (by default). Change to 10 folds and repeat.\n",
    "- Q7: What is the \"score\" being reported? \n",
    "- Q8: Without running code, [what do you think our model is currently scoring for precision, sensitivity, and recall?]\n",
    "- Q9: Change the scoring method to one of those. [Specify them in sklearn is here.](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter). How did your score change and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6dae37-794e-4309-afd8-7ee674b50b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffdde4-97f8-400b-8c94-24a06b4f419c",
   "metadata": {},
   "source": [
    "### Step 6: Optimizing the model\n",
    "\n",
    "### Aka \"tuning the hyperparameters\"\n",
    "\n",
    "It seems like something about how the default values of the logit model are set is leading to a simpleton model: Predict every loan is paid back!\n",
    "\n",
    "So, the idea here is to repeat the CV above, many times, with different parameter values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c825c6-ebb8-4dd6-9c8d-453cc20bb9c5",
   "metadata": {},
   "source": [
    "### What hyperparameters can I change?\n",
    "\n",
    "The easiest way to see all the hyperparameters in the pipeline is this: \n",
    "\n",
    "`<pipename>.get_params()`\n",
    "\n",
    "Notice how the \"C\" parameter for the LogisticRegression function is called \"`logisticregression__C`\" below? We will come back to that in  a second!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201b48c-762a-4066-98bf-e90fd628d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f1b70-ffa9-4d30-9358-eb7a10a21d03",
   "metadata": {},
   "source": [
    "### Setting up the \"hyperparameter grid\"\n",
    "\n",
    "The combination of parameters you want to try is\n",
    "- a dictionary\n",
    "- the **keys** in the dict are the hyperparameters you want to change (specifically, how the parameter is **named in the pipeline**\n",
    "- the **values** in the dict are the values for that hyperparameter you want to change\n",
    "\n",
    "For example:\n",
    "```python\n",
    "parameters =  {'logisticregression__C': [0.001,0.1,1,5]}\n",
    "```\n",
    "\n",
    "The reason I wrote the weird \"`logisticregression__C`\", is because this will help `sklearn` find the function and its parameter to change. This means that within the called \"logisticregression\" (followed by two underscores) there is a parameter called \"C\".\n",
    "\n",
    "Similarly, within the \"columntransformer\" step, there is a \"num_impute\" step, which has a \"simpleimputer\" step, which is a function that has a parameter called \"strategy\".\n",
    "\n",
    "Thus, if you want to try other strategies for filling blank numbers in, \"`'columntransformer__num_impute__simpleimputer__strategy'`\" is what you need.\n",
    "\n",
    "Thus:\n",
    "```python\n",
    "parameters =  {'logisticregression__C': [0.001,0.1,1,5],\n",
    "              'columntransformer__num_impute__simpleimputer__strategy' : ['mean','median']}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c64759-217a-48fe-acc2-80e60644c7d1",
   "metadata": {},
   "source": [
    "### All together now...\n",
    "\n",
    "1. Set up your parameter grid\n",
    "1. Make a \"super estimator\" object with `GridSearchCV`. This just runs cross_validate for every combination of parameters in the grid. (Below, 2x3=6 combinations.)\n",
    "1. Just like CV, use `.fit()` to run the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c126e83-44f9-4544-a87f-0aebc9c21241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyper param grid - what params in a pipeline do you want to change?\n",
    "# a dictionary. keys are things to change in pipeline, values are what to try for that param\n",
    "# key: <stepname>__<parametername>\n",
    "\n",
    "parameters =  {'logisticregression__C': [0.1,1,5], \n",
    "              'columntransformer__num_impute__simpleimputer__strategy' : ['mean','median']}\n",
    "\n",
    "#     find optimal hyper params (gridsearchcv)\n",
    "\n",
    "grid_search = GridSearchCV(estimator = logit_pipe, \n",
    "                           param_grid = parameters,\n",
    "                           cv = 10\n",
    "                           )\n",
    "\n",
    "results = grid_search.fit(X_train,y_train)\n",
    "\n",
    "#     save pipeline with optimal params in place\n",
    "#     (Note: you should spend time interrogating model predictions, plotting and printing.\n",
    "#     Does the model struggle predicting certain obs? Excel at some?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b90668-1012-4b05-ace5-b8f738b70d8f",
   "metadata": {},
   "source": [
    "### What to do after that?\n",
    "\n",
    "[The \"outputs\" of the grid search are the attributes of the results object, as listed here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV)\n",
    "\n",
    "- Q10: Output the CV results\n",
    "    - Bonus: Do it as a dataframe\n",
    "- Q11: Make the grid search output recall, sensitivity, f1, and accuracy \n",
    "\n",
    "---\n",
    "\n",
    "Now, here is what I'd _**like**_ to ask you:\n",
    "\n",
    "- Q12: Which of these models would you choose, taking into account the bias-variance tradeoff? Discuss whether these models are high bias or not, and whether they are high variance or not.\n",
    "- Q13: Outline how we might adjust our model here to improve its performance\n",
    "\n",
    "**Except: Don't answer those for today's (bad) logit model. I've \"hidden\" something from you about why this model is performing so poorly. Think of this as \"a case study in miniature\" about how \"black boxes\" can make dealing with ML models impenetrable.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093510f4-beaf-468c-b891-fd1ea6a48311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put answers here for Q10 and Q11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
