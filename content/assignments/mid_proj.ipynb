{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm aka Assignment 5 - Our first real data science project\n",
    "\n",
    "```{admonition} Tips\n",
    ":class: tip\n",
    "1. Read all instructions before starting.\n",
    "1. Start early. Work on the components of the project in parallel with related class discussions.\n",
    "1. RECHECK THESE INSTRUCTIONS BEFORE SUBMITTING\n",
    "1. Look at my class notes repo. I worked on part of this project there, especially step 3 and parts of step 6.\n",
    "```\n",
    "\n",
    "```{warning}\n",
    "Per the [syllabus](../about/gradeoverview), this project is 10% of your overall grade, which is 3x the weight of a typical assignment. It will take 2-3x the time of a typical assignment.\n",
    "\n",
    "**Really fun news:** This is a end-to-end data science project! You will be downloading a lot of files, parsing/exploring/cleaning those file, and then exploring the data. \n",
    "\n",
    "BUT: It will take time! If you start the day before it is due, YOU WILL NOT FINISH IT. If you start two days before it is due, you might finish it, but it will not be done well.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Set Up\n",
    "\n",
    "First, let's start with a high level overview. (More details will follow.)\n",
    "\n",
    "- Basic question: Do 10-K filings contain value-relevant information in the **sentiment** of the text?\n",
    "    - Sentiment: Does the word have a positive or negative tone (e.g. \"confident\" vs \"against\")\n",
    "- Specific questions: Is the positive or negative sentiment in a 10-K associated with better/worse stock returns?\n",
    "    - This is called a \"cross-sectional event study\"\n",
    "    - Expected minimum output: Scatterplot (x = document's sentiment score, y = stock returns around the 10-K's release) with regression lines; formatted well\n",
    "    - Advanced output: Regression tables, heatmaps, better scatterplots/similar\n",
    "- New data science technique: Textual analysis and sentiment analysis. \n",
    "- Data needed:\n",
    "    - Returns: Stock returns for firms around their 10-K dates\n",
    "    - Sentiment measures will be created from the text in their 10-K filings. (We will need to download the 10-K files and get the text within them!)\n",
    "    \n",
    "_Notice: The questions dictates what the analysis design should be, which in turn dictates the data requirements. Question led research > data driven research._\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project deliverables\n",
    "\n",
    "Your repo and the code within should put the Golden Rules from Chapters 2, 3.3, and 4 into practice. You should review those. You will be graded on them in following ways (not an exhaustive list): \n",
    "\n",
    "- The repo's structure (files, organization, names, README)\n",
    "- Code style/clarity in the `download` and `build_sample` files (_The teaching team will **not** read your `build_sample` file other than to grade/comment on code style and, potentially, errors. Answers to questions here will be ignored - put them in the report file!_)\n",
    "- **The main portion of your grade is the `report` file. Your answers to all questions, unless explicitly directed otherwise, should be in the `report` file. The `report` file should be written and formatted like an executive report.** \n",
    "    - It is not a technical document and not a bunch of hasty code. It should be well formatted and clean in terms of text, code, and output. Don't show extraneous print statements you wrote while ABCD'ing and checking your code. Treat it like a Word document that happens to have some code (but just enough to do the analysis and show outputs). \n",
    "    - There is no \"page expectation\" or \"page limit\". Aim to provide sufficient analysis and explanation but in a concise and clear way. Bullet points are fine in places, but you should have a few places with paragraph-style discussion.\n",
    "    - High quality and concise (but sufficient) reporting is an A1 emphasis. **Here, pretty, smart, and effective tables and visualizations will receive higher grades.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to complete the assignment\n",
    "\n",
    "````{margin}\n",
    "```{tip}\n",
    "It may make sense to read this backwards. Knowing what I want in the report dictates what I need to do to build the sample.\n",
    "```\n",
    "````\n",
    "\n",
    "```{dropdown} 1. Start the assignment \n",
    "\n",
    "- As usual, click the link I provide in the discussion board. \n",
    "- The repo is almost empty. This is a start-to-finish project, so I'm letting you dictate the structure of the files.\n",
    "- I have provided some input files and two papers that are relevant to the midterm\n",
    "- Clone this to your computer.\n",
    "```\n",
    "\n",
    "````{dropdown} 2. Edit **.gitignore** \n",
    "\n",
    "The `get_text_files_zipAFTER.ipynb` file will create a large data structure in a subfolder called `10k_files/` with all the downloaded 10-K files. Depending on the choices you/we make, there could be several gigs of data in this folder. We don't want to save/push all these files to GitHub!\n",
    "\n",
    "```{warning}\n",
    "Add this directory (`10k_files/`) to your gitignore before you proceed!\n",
    "```\n",
    "\n",
    "````\n",
    "\n",
    "\n",
    "````{dropdown} 3. Create **get_text_files_zipAFTER.ipynb** \n",
    "This file \n",
    "1. Should create a subfolder for inputs (`inputs/`), if one doesn't exist. We will save background info about our sample firms there.  \n",
    "1. Should create another subfolder (`10k_files/`) to hold all the text files you download. Because scraping can generate large amounts of files, I usually put it in a dedicated input folder instead of the generic input folder we just made.\n",
    "1. If you don't already have it, get a list of firm tickers for our study and download their last 10-K filed during 2022.\n",
    "1. Download the last 10-K filed during 2022 for each firm, if you haven't already downloaded that one. \n",
    "\n",
    "**Tips/recommendations**\n",
    "\n",
    "1. Try to download just one 10-K at first. When you can successfully do that, try a few more, one at a time. Check the folders on your computer - did they download like you expected? Are the files correct? If yes, continue. If not, you have an error to fix. \n",
    "1. The website has really good info on \"building a spider.\" Highly recommend! \n",
    "1. We will spend time in class building this file up. Follow along!\n",
    "\n",
    "```{tip}\n",
    "When you are confident the program works, \n",
    "1. Delete your whole `10k_files/` subfolder on your computer to make sure the program works from a \"fresh start\" \n",
    "2. Rerun this from scratch. \n",
    "3. Rerun the file AGAIN (but don't delete the files you have). The file should not download any new files! Does the file work after it's already been run, or partially completed its work? Real spiders have to resume where they left off. You might need to make some conditional tweaks to the file to account for this. You don't want the code to actually re-download the data, but the code should still run without error!\n",
    "```\n",
    "\n",
    "````\n",
    "```{dropdown} 4. IMPORTANT: Create **screenshot.png**\n",
    "\n",
    "It's not polite to upload so much data to GitHub. It takes up space on the server, and your collaborators/peer reviewers will have to download them all when they clone your repo. \n",
    "\n",
    "That's why you edited the gitignore before doing all those downloads. If you did it correctly and check GitHub Desktop, you won't see any of the text files!\n",
    "\n",
    "1. Now that your `get_text_files_zipAFTER.ipynb` is done running, push the repo. Even though your _computer_ has a `/10k_files/*` folder on it with many files and some hard drive space used, the repo in your browser doesn't show this at all! Good job!\n",
    "2. **Create `screenshot.png`. The purpose of this is to upload proof of the files for your reviewers.**\n",
    "\n",
    "Right-click your `10k_files` folder so it shows the number of files inside of it, and take a screenshot showing this. Save it as `screenshot.png` inside your repo. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{dropdown} 5. Download **near_regex.py** from the [community codebook](https://github.com/LeDataSciFi/ledatascifi-2024/tree/main/community_codebook) into your repo\n",
    "\n",
    "This will be used in the next step.\n",
    "\n",
    "Pro tip: Delete everything in this file below line 118.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{dropdown} 6. Create **build_sample.ipynb**\n",
    "\n",
    "The idea is that this code builds everything we need for the report. The report should be minimal (just the necessary code to produce the output tables and figures you will discuss) so that readers can focus on making conclusions. So, this file should create a dataset that has\n",
    "- one observation per firm \n",
    "- variables needed for the analysis\n",
    "\n",
    "```{important}\n",
    "I know what should be in this file because I worked _backwards_ from what I know I want in the report. If you're reading this section and are confused, go look at the report section below.\n",
    "```\n",
    "\n",
    "**Variables to create:**\n",
    "1. 3 versions of a \"buy and hold\" around the 10-K date (\"date t\")\n",
    "    - Version 1: The return on the day of the filing. (This is easiest, and points will be reserved for students that figure out version 2 and 3.)  \n",
    "    - Version 2: Measure from the day t to day t+2 (inclusive) ... `t` is business days, so ignore weekends and holidays!\n",
    "    - Version 3: Measure from the day t+3 to day t+10 (inclusive)\n",
    "    - Calculate the firms's buy and hold return over each time span, a la Assignment 2\n",
    "    - Stock returns for 2022 [can be found here.](https://github.com/LeDataSciFi/data/tree/main/Stock%20Returns%20(CRSP)) Open this the same way we open `ccm`.\n",
    "    - Merge the buy and hold stock returns to our dataset using ticker. (This is not the best option, but it's simple.) \n",
    "1. 10 sentiment variables \n",
    "    - A positive sentiment variable is the fractions of words in a 10-K that are \"positive\" words \n",
    "    - A negative sentiment variable is the fractions of words in a 10-K that are \"negative\" words \n",
    "    - How do we define which words are positive and which are negative? The input folder contains two different sets of words. \n",
    "        - The \"LM\" sentiment dictionary comes from two researchers named Loughran and McDonald\n",
    "        - The \"ML\" sentiment lists comes from a machine learning approach used in the Journal of Financial Economics this year\n",
    "        - Background on both measures: You should read the abstract and the intro of the papers in the literature folder. Because \"ML_JFE.pdf\" is the more recent publication, it contrasts the two dictionaries\n",
    "    - The first 4 of the 10 variables:\n",
    "        - \"LM Positive\" and \"LM Negative\"\n",
    "        - \"ML Positive\" and \"ML Negative\"\n",
    "    - The last 6 of the 10 variables: \"Contextual\" sentiment. Basically: What is the (positive and negative) sentiment of the text in a 10-K around discussions of a particular topic.\n",
    "        - Pick three topics. Each of those will get a positive and negative sentiment score - use the ML sentiment lists (not the LM lists).\n",
    "        - [A must visit: This page explains this step.](contextual_sentiment)\n",
    "\n",
    "**This file (broad steps):**\n",
    "1. Creates an `output/` folder \n",
    "1. Loads the initial dataset of sample firms saved inside of `inputs/`.\n",
    "1. For each firm, \n",
    "    - load the corresponding 10-K. Clean the text.\n",
    "    - Create the sentiment measurements, and save those new measurements to the correct row and column in the dataframe.\n",
    "    - Bonus: Save the total length of the document (# of words)\n",
    "    - Bonus: Save the # of unique words (similar to total length)\n",
    "1. Calculate the return measurements. Save those to the correct row and column in the dataframe\n",
    "1. Optional: Downloads 2021 accounting data (**2021 ccm_cleaned.dta**) from the data repo (possibly useful in analysis) and adds them to the dataset \n",
    "1. Save the whole thing to `output/analysis_sample.csv`\n",
    "\n",
    "```{note}\n",
    "[There is more details about the 6 \"contextual sentiment\" variables here.](contextual_sentiment)\n",
    "```\n",
    "\n",
    "```{important}\n",
    "When you are confident the program works, delete your whole `output/` folder on your computer so you have a \"fresh start\" and then rerun this from scratch. \n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{dropdown} 7. Create **exploration_ugly.ipynb**.\n",
    "\n",
    "Use this file to try to figure out how to do the analysis below. Play around in this file. No one will look at it. It's a safe space.\n",
    "\n",
    "If you find issues with your sentiment measurements or come up with improvements you think you should make, go back and work on the previous file more. \n",
    "\n",
    "You can and should use this file to figure out what you want to include in the final report and how you want it to appear.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{dropdown} 8. Create **report.ipynb** \n",
    "\n",
    "```{important}\n",
    "This is the main portion of your grade!!!\n",
    "```\n",
    "\n",
    "The following outputs and discussion prompts are the focus of grading:\n",
    "\n",
    "1. Summary section (brief, max 2 paragraphs)\n",
    "    - Summarize your question, what you did, and your findings. You can model this on the abstracts in the literature folder. \n",
    "1. Data section\n",
    "    - What's the sample?  \n",
    "    - How are the return variables built and modified? (Mechanical description.) Aim for rigor and do not skip steps. You can include text and formulas here. \n",
    "    - How are the sentiment variables are built and modified? (Mechanical description.) Aim for rigor and do not skip steps. You can include text and formulas here. \n",
    "    - These datapoints about the sentiment variables: \n",
    "        - How many words are in the LM positive dictionary?\n",
    "        - How many words are in the LM negative dictionary?\n",
    "        - How many words are in the ML positive dictionary?\n",
    "        - How many words are in the ML negative dictionary?\n",
    "        - A description of how you set up the near_regex function (partial = true or false, distance = what) and why you chose the values you did.\n",
    "    - Why did you choose the three topics you did for the \"contextual sentiment\" measures?\n",
    "    - Show and discuss summary stats _of your final analysis sample_\n",
    "    - Do your  \"contextual sentiment\" measures pass some basic smell tests?\n",
    "        - Smell tests: Is something fishy? (What you look for depends on the setting.)\n",
    "        - Do you have variation in the measures (i.e a variable is not all the same value)?\n",
    "        - Are industries the industries you expect talking about your subject positively or negatively?\n",
    "    - This should be sufficient for a reader to understand the dataset and have context for interpreting results\n",
    "    - Are there any caveats about the sample and/or data? If so, mention them and briefly discuss possible issues they raise with the analysis.     \n",
    "1. Results\n",
    "    - Make a table with the correlation of each (10) sentiment measure against both (2) return measures. (So: a 10x3 table.)\n",
    "        - The return measures are the firm's returns around the 10-K release. We have 3 versions of this to examine different windows of time around the 10-K to see the speed at which information is priced into the stock.  \n",
    "        - You will make 5 sentiment measures, and each has a positive and negative component. Thus: 10 sentiment measures.  \n",
    "        - _See step 6 for more details about the sentiment and return measures._\n",
    "    - Include a scatterplot (or binscatter, **or similar**) of each sentiment measure against both return measures. \n",
    "        - Better: Combining this into a single figure \n",
    "        - Better: Skip the correlation table and include the numerical correlations on the figure\n",
    "        - Better: Regress (Don't worry about printing these regressions out \"pretty\", just try them if you want!). You can including accounting variables as controls here. \n",
    "    - Four discussion topics:\n",
    "        - On (1), (2), and (3) below: Focus just on the first return variable (_which will examine returns around the 10-K publication_)\n",
    "        - On (4) below: Focus on how the \"ML sentiment\" variables (positive and negative) are related to the different return measures. \n",
    "        \n",
    "        1. Compare / contrast the relationship between the returns variable and the two  \"LM Sentiment\" variables (positive and negative) with  the relationship between the returns variable and the two \"ML Sentiment\" variables (positive and negative). Focus on the patterns of the signs of the relationships and the magnitudes.\n",
    "        1. If your comparison/contrast conflicts with Table 3 of the Garcia, Hu, and Rohrer paper (ML_JFE.pdf, in the repo), discuss and brainstorm possible reasons why you think the results may differ. If your patterns agree, discuss why you think they bothered to include so many more firms and years and additional controls in their study? (It was more work than we did on this midterm, so why do it to get to the same point?)\n",
    "        1. Discuss your 3 \"contextual\" sentiment measures. Do they have a relationship with returns that looks \"different enough\" from zero to investigate further? If so, make an economic argument for why sentiment in that context can be value relevant.\n",
    "        1. Is there a difference in the sign and magnitude? Speculate on why or why not. \n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{dropdown} 9. Finalize and polish\n",
    "\n",
    "Unlike previous assignments, how clean your code and report are will factor into your grade. Additionally, your README file should be nice!\n",
    "\n",
    "**Edit the readme file - it should be \"publication ready\"**\n",
    "- Make the readme file informative and professional. Use headers to separate sections within. \n",
    "- Inform readers of the order in which files should be run. And warn users that this folder will download X files of X MB or GB.\n",
    "- Change the title of it (not the filename, the title at the top)\n",
    "- Describe the purpose of this repo (what this repo is analyzing) and the key inputs\n",
    "- List any necessary packages (might a reader need to `pip install` anything?) or steps a visitor will need to run to make it work on their computer  \n",
    "    \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheers!\n",
    "\n",
    "**Give yourself a big round of applause at this point!**\n",
    "\n",
    "Your code is probably very flexible and powerful at this point. If you have the appetite + a larger list of EDGAR files to download + a large enough hard drive + and time, then you could download more than 100GB of 10-K filings and run textual analysis across 25+ years of data for all publicly traded firms. \n",
    "\n",
    "Seriously: You are in the ballpark of pulling off any analysis you want that needs to harness the power of SEC filings. These four studies are variously provocative, great, and (in one case) mine:\n",
    "- [Check this claim: Identifying changes in 10-K/Q filings can generate a 20% alpha](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1658471)\n",
    "- [Prof. Hanley measured emerging risks in the financial sector](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2792943)\n",
    "- [Build a unique list of competitors for each firm (really powerful!)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1520062)\n",
    "- [I used 10-K text to identify public rivals of startup firms](https://ssrn.com/abstract=3245839)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
