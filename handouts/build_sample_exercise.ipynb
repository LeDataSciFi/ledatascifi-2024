{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265015d0",
   "metadata": {},
   "source": [
    "## Warning\n",
    "\n",
    "Some students copy this file into their assignment repo. DON'T!!! \n",
    "\n",
    "Why? This file is just exercises that help you build that file. Here, we are solving little problems you'll have as you write it. \n",
    "\n",
    "When you create `build_sample.ipynb`, do it from scratch, put the psuedocode structure in place, and proceed from there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af7d26-85de-44f3-83ac-e98340ac2d0f",
   "metadata": {},
   "source": [
    "## First\n",
    "\n",
    "- Copy `NEAR_regex.py` into the same folder as this file. [It's here](https://ledatascifi.github.io/ledatascifi-2024/content/04/02d_RegexApplication.html#demo) (click the \"+\") or in the community codebook. You should name the file `NEAR_regex.py` and not `NEAR_regex.ipynb`.\n",
    "- Make a `.gitignore` file in this folder with  `**10k_files/*` in it.\n",
    "- Copy [this file](https://github.com/donbowen/Class-Notes-1045/raw/main/Midterm%20sandbox/10k_files.zip) into the `10k_files/` folder here.\n",
    "- Copy the things in the assignment's input folder in to the inputs folder here.\n",
    "- Optional: You can install `tqdm` (If you don't, then remove it from the code below.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e0c86-ca84-47f8-95e7-e34c024ed2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from time import sleep\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from near_regex import NEAR_regex  # copy this file into the asgn folder\n",
    "from tqdm import tqdm  # progress bar on loops\n",
    "\n",
    "# if you have tqdm issues, run this in terminal or with ! trick\n",
    "# jupyter nbextension enable --py widgetsnbextension\n",
    "# jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "#\n",
    "# if that fails, you can disable it\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9fec2c-a2bd-4513-9430-33fd2401cdec",
   "metadata": {},
   "source": [
    "## Load sentiment dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c226f-b543-4c03-a3d6-0dc1c77c3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 - load the ML negative words into a list called BHR_negative\n",
    "# BHR is the author names on that paper\n",
    "# \"ML\" might be a better name, but having \"LM\" and \"ML\" in bound\n",
    "# to cause transcription errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ada34b-0a53-4101-ba01-c7701948300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 - load the ML positive words into a list called BHR_positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801cd7f8-93b9-4ee8-b629-f3de587b4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 - load the LM negative words into a list called LM_positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3bfea-9769-4713-9243-d47f546881e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 - load the LM positive words into a list called LM_positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcdd66-e646-4a64-bf26-a30092d1d970",
   "metadata": {},
   "source": [
    "## Looping over a dataframe and adding a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894af2c-986e-4ec5-b642-96ca502f5b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# step 1 will load some database and prep it for the loopy parts\n",
    "# here, we will just use a toy dataset\n",
    "\n",
    "toy_database = pd.DataFrame({\"Security\":['3M','TLSA','APPL'],\n",
    "             \"URL\":['blahblah.com','wikisomething.com','wiki.com']})\n",
    "\n",
    "# step 2: figure out how to loop through this dataframe \n",
    "# yes, an actual for loop on a dataframe (booooooo)\n",
    "for ______________________________: \n",
    "    # print the row's index, and the url from the row, this will confirm if we are looping right\n",
    "    print(_________________________) \n",
    "\n",
    "    # A. here, you would open the related 10k, but SKIP this for now\n",
    "\n",
    "    # B. You'd measure the sentiment here. Let's just pretend that \n",
    "    # you opened+cleaned+built a sentiment variable \n",
    "    # called \"sentiment_positive\" (a bad name, but this is just example code!)\n",
    "\n",
    "    sentiment_positive= random.randint(0,10) # this is a silly line to \"simulate\" that got a value for this variable\n",
    "\n",
    "    __________________________ # add sentiment_positive to your toy database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f890f76-b14b-43af-bc30-945d8300d48e",
   "metadata": {},
   "source": [
    "## Measure sentiment\n",
    "\n",
    "What fraction of the words in this \"document\" (sentence) are \"happy\" words?\n",
    "\n",
    "Answer: 2/13. Let's replicate that with code. \n",
    "\n",
    "First, count the length of the document.\n",
    "\n",
    "Then, count how many times each word is in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e4fb0-1b06-4406-9127-545cbd3ef723",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_sentiment = ['happy','smile','hopeful']\n",
    "\n",
    "sentence = 'I am happy that you are here. I am all smiles. So hopeful!'\n",
    "\n",
    "# q0 count the number of \"words\" (the doc length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70054e4-4ef9-47d3-ae7d-aabff017a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1 count how many times \"happy\" is used in the doc\n",
    "# hint: https://ledatascifi.github.io/ledatascifi-2024/content/04/02b_regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4a25d-c2af-4d6f-aa45-d457492be2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q2 count how many times \"smile\" is used in the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee35ab-cf0d-4c52-b294-70f5a8c1344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q3 count how many times \"smile\" or \"happy\" is used in the doc\n",
    "# hint: similar to q2 answer... \n",
    "# the answer is somewhere this page: https://regexone.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194d52c-6b7a-456e-8b2a-9972476b6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q4 - prof demo - count how many time all the words are in the doc\n",
    "# 4.4.4 has examples + output\n",
    "# docstring: https://github.com/LeDataSciFi/ledatascifi-2024/blob/main/community_codebook/near_regex.py\n",
    "# solve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42fa877-20a1-4f7f-890a-fb55f84b9589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q5 - using py's string functions, convert\n",
    "# happy_sentiment into the format NEAR_regex() wants \n",
    "# hint: 4.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c738e28-d4ad-49ea-8872-3bb3f6fbdabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q6 - calculate the doc's happy_sentiment score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9906de8-c99f-4651-9cfb-2a7ad8f4cc60",
   "metadata": {},
   "source": [
    "## Opening a 10-K file\n",
    "\n",
    "I'm giving everyone this code because dealing with Zips is a headache the first 15 times you do it.\n",
    "- Open the zip before the loop and get a list of all files already in it\n",
    "- With that zip open, do your loopy stuff inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c63770-1ea8-4e97-96aa-03030e4751b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the zip file (do this before the for loop\n",
    "# so you only open it one time... faster)\n",
    "with ZipFile('10k_files/10k_files.zip','r') as zipfolder:\n",
    "    \n",
    "    # before the loop, get list of files in zipped folder\n",
    "    file_list = zipfolder.namelist()\n",
    "        \n",
    "    # replace this with how you'd loop over the dataframe\n",
    "    for firm in [\"ALLE\"]: #,\"MMM\",\"TSLA\"]:\n",
    "        \n",
    "        # get a list of possible files for this firm\n",
    "        firm_folder    = \"sec-edgar-filings/\" + firm + '/10-K/*/*.html'\n",
    "        possible_files = fnmatch.filter(file_list, firm_folder) \n",
    "        if len(possible_files) == 0: \n",
    "            continue\n",
    "            \n",
    "        fpath = possible_files[0] # the first match is the path to the file\n",
    "\n",
    "        # open the file (this is a little different!)\n",
    "        with zipfolder.open(fpath) as report_file:\n",
    "            html = report_file.read().decode(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9633d1-6074-4f45-a207-ec1e2633ea6f",
   "metadata": {},
   "source": [
    "## Cleaning the html\n",
    "\n",
    "Print out `html`... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9ce50-afea-4414-8fc1-71499a31aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "html[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ebf63d-588b-4b5c-8834-0d04019230f7",
   "metadata": {},
   "source": [
    "Regex won't work on this as is! We need to remove all the html tags, drop the hidden data, and then, with the remaining text, clean it up using the \"Good ideas\" in 4.4 and 4.4.4 of the book. However, we have to slightly adjust the code. \n",
    "\n",
    "1. Use BeautifulSoup() with the `lxml-xml` parser. Call the output `soup`. Don't use `get_text` yet. \n",
    "1. Delete the hidden XBRL \n",
    "\n",
    "    ```python\n",
    "    for div in soup.find_all(\"div\", {'style':'display:none'}): \n",
    "        div.decompose()\n",
    "    ```\n",
    "    \n",
    "1. Continue on (get the text from the soup, and continue from there...)\n",
    "1. Check: My cleaned string says \"allegion\" in positions 396-404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ad006-18ee-4a08-b826-ac8a4c1e61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b70af-c46c-47dd-85f7-31c56c7f0cec",
   "metadata": {},
   "source": [
    "## Get 10-K dates\n",
    "\n",
    "We need to know when the 10-K is released to see the stock returns around it.\n",
    "\n",
    "I'm going to give you most of this code. How I figured it out:\n",
    "- I know we have the CIK and accession number\n",
    "- Looked for EDGAR urls that have CIK + accession number, and then list filing date on the page\n",
    "- https://www.sec.gov/Archives/edgar/data/1122304/0001193125-15-118890-index.html\n",
    "- `requests_html` ([my listed suggestion here](https://ledatascifi.github.io/ledatascifi-2024/content/04/01_Intro_to_scraping.html#my-suggestion)) is the `requests` module for getting data from the web PLUS the ability to grab parts of the html\n",
    "    \n",
    "Exercise:\n",
    "- I used code straight off the [documentation's home page](https://requests.readthedocs.io/projects/requests-html/en/latest/), adapted slightly. Look for examples that _find_ parts of the html.\n",
    "- You'll need to figure out the \"CSS Selector\"\n",
    "    - right click on the filing date on the webpage, click inspect\n",
    "    - in the area that popped up, right click on html code containing that date and copy the CSS selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b5286-1c2a-490c-aa1a-179d5cc22700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before the loop, set up a browser session\n",
    "\n",
    "from requests_html import HTMLSession\n",
    "session = HTMLSession()\n",
    "\n",
    "# inside your loop, get the cik and accession number for the filing\n",
    "\n",
    "cik = 1122304\n",
    "accession_number = '0001193125-15-118890'\n",
    "\n",
    "# *one* way to get the filing date... \n",
    "\n",
    "url = f'https://www.sec.gov/Archives/edgar/data/{cik}/{accession_number}-index.html'\n",
    "print(url) # check it out...\n",
    "r = session.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9307f137-fa28-47d4-a45c-ff5b42d9d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: get the filing date out of this \"r\" object (one line of code will do)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34a7d5-f345-49df-93c3-13d46ad1a160",
   "metadata": {},
   "source": [
    "To use this in your actual midterm, save the  accession_number to your database while doing the main for loop to parse the text.\n",
    "\n",
    "Then, after that, I wrote a second for loop that loops over the rows, and uses the code above to grab the date. I added some error checking (What if we don't have an accession number for that firm, what if the url is wrong, or the server denies you, or the line of code with filing_date fails?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0af1b2-4d57-448b-8673-3e20d5cb2b50",
   "metadata": {},
   "source": [
    "## Get returns around the 10-K dates\n",
    "\n",
    "[Returns for 2022 are here.](https://github.com/LeDataSciFi/data/blob/main/Stock%20Returns%20(CRSP))\n",
    "\n",
    "Before you try to use that, here are toy datasets of returns and filing dates that mimic the structure of the data you'll actually have. \n",
    "\n",
    "What is the [0,2] and [3,10] cumulative returns for each firm? It's easy to actually figure out! Doing so will help you with the pseudo, and in any case... how can you know if you're right otherwise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2826dd22-cb38-4680-9bbe-317291406f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'ticker': ['JJSF']*20 + ['TSLA']*20,\n",
    "    'date': ['2021-12-01', '2021-12-02', '2021-12-03', '2021-12-06', '2021-12-07', '2021-12-08', '2021-12-09', '2021-12-10', '2021-12-13', '2021-12-14', '2021-12-15', '2021-12-16', '2021-12-17', '2021-12-20', '2021-12-21', '2021-12-22', '2021-12-23', '2021-12-27', '2021-12-28', '2021-12-29'] + ['2022-12-02', '2022-12-05', '2022-12-06', '2022-12-07', '2022-12-08', '2022-12-09', '2022-12-12', '2022-12-13', '2022-12-14', '2022-12-15', '2022-12-16', '2022-12-19', '2022-12-20', '2022-12-21', '2022-12-22', '2022-12-23', '2022-12-27', '2022-12-28', '2022-12-29', '2022-12-30'],\n",
    "    'ret': [-0.011276, 0.030954, 0.000287, 0.014362, 0.012459, 0.017200, -0.010173, 0.011875, 0.012559, 0.002508, 0.022852, 0.012360, 0.017387, -0.008957, 0.016840, -0.000256, -0.002558, 0.009041, -0.002097, 0.010189] + [0.000822, -0.063687, -0.014415, -0.032143, -0.003447, 0.032345, -0.062720, -0.040937, -0.025784, 0.005548, -0.047187, -0.002396, -0.080536, -0.001669, -0.088828, -0.017551, -0.114089, 0.033089, 0.080827, 0.011164]\n",
    "}\n",
    "\n",
    "crsp = pd.DataFrame(data)\n",
    "crsp['date'] = pd.to_datetime(crsp['date'])\n",
    "\n",
    "fake_filings = pd.DataFrame({'ticker':['JJSF','TSLA'],\n",
    "                             'filing_date':['2021-12-03','2022-12-13']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8b74f-0d1c-4e4a-bf3a-9e598f91cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try here...\n",
    "\n",
    "# pseudocode first! imagine the structure of the dataset you want and work backwards, you'll struggle otherwise!\n",
    "\n",
    "# this really is a paper and pencil problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbce527-c804-4998-9d6b-ec69d32a0588",
   "metadata": {},
   "source": [
    "## Merge it all together\n",
    "\n",
    "The readme shows what the output dataset should look like, roughly. The midterm directions elaborate (10 sentiment variables, 2 return measures). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf6bb1-d32e-4ec9-a797-06c84556436c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
